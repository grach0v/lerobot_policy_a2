"""Configuration for A2 Policy (ViLGP3D)."""
from dataclasses import dataclass, field
from typing import Optional

from lerobot.configs.policies import PreTrainedConfig
from lerobot.configs.types import NormalizationMode
from lerobot.optim.optimizers import AdamWConfig
from lerobot.optim.schedulers import LRSchedulerConfig


@PreTrainedConfig.register_subclass("a2")
@dataclass
class A2Config(PreTrainedConfig):
    """Configuration class for A2Policy (ViLGP3D).

    This policy uses CLIP-based cross-attention for 6-DOF grasp and place
    pose selection from candidate actions generated by GraspNet/PlaceNet.
    """

    # Policy identification
    name: str = "a2"

    # Observation settings
    n_obs_steps: int = 1

    # Action settings
    action_dim: int = 7  # xyz (3) + quaternion (4)

    # Network architecture
    width: int = 768
    layers: int = 1
    heads: int = 8

    # Position encoding
    use_rope: bool = True
    no_feat_rope: bool = False

    # Point cloud processing
    sample_num: int = 500

    # Feature backbone
    feat_backbone: str = "clip"

    # Fusion settings
    fusion_sa: bool = False
    no_rgb_feat: bool = False

    # GraspNet checkpoint path
    graspnet_checkpoint: Optional[str] = None

    # Device
    device: Optional[str] = None

    # Normalization mapping for different modalities
    # A2 uses IDENTITY (no normalization) by default since it operates on raw poses
    normalization_mapping: dict = field(
        default_factory=lambda: {
            "VISUAL": NormalizationMode.IDENTITY,
            "STATE": NormalizationMode.IDENTITY,
            "ACTION": NormalizationMode.IDENTITY,
        }
    )

    # Optimizer settings
    optimizer_lr: float = 1e-4
    optimizer_weight_decay: float = 1e-4

    # Input/output features (will be set based on environment)
    input_features: dict = field(default_factory=dict)
    output_features: dict = field(default_factory=dict)

    def __post_init__(self):
        super().__post_init__()

    def validate_features(self) -> None:
        """Validate input/output feature compatibility."""
        pass

    @property
    def observation_delta_indices(self) -> None:
        """No temporal delta for observations (single-step policy)."""
        return None

    @property
    def action_delta_indices(self) -> list:
        """Single action output (no chunking)."""
        return [0]

    @property
    def reward_delta_indices(self) -> None:
        """No reward prediction."""
        return None

    def get_optimizer_preset(self) -> AdamWConfig:
        """Return default optimizer configuration."""
        return AdamWConfig(
            lr=self.optimizer_lr,
            weight_decay=self.optimizer_weight_decay,
        )

    def get_scheduler_preset(self) -> LRSchedulerConfig | None:
        """No learning rate scheduler by default."""
        return None
